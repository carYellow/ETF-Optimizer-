<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Stock Prediction Model Training Report</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/css/bootstrap.min.css" rel="stylesheet">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            padding: 20px;
            max-width: 1200px;
            margin: 0 auto;
            background-color: #f9f9f9;
        }
        h1, h2, h3, h4, h5, h6 {
            color: #2c3e50;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }
        h1 {
            color: #1a237e;
            border-bottom: 2px solid #1a237e;
            padding-bottom: 10px;
        }
        h2 {
            border-bottom: 1px solid #ddd;
            padding-bottom: 5px;
        }
        .table {
            width: 100%;
            margin-bottom: 2rem;
            border-collapse: collapse;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
            overflow: hidden;
        }
        .table th {
            background-color: #2c3e50;
            color: white;
            text-align: left;
            padding: 12px;
            font-weight: bold;
        }
        .table td {
            padding: 10px 12px;
            vertical-align: middle;
            border-top: 1px solid #ddd;
        }
        .table-striped tbody tr:nth-of-type(odd) {
            background-color: rgba(0, 0, 0, 0.03);
        }
        .table-hover tbody tr:hover {
            background-color: rgba(0, 0, 0, 0.075);
        }
        .code-block {
            background-color: #f8f9fa;
            border: 1px solid #eaecef;
            border-radius: 6px;
            padding: 16px;
            overflow-x: auto;
            font-family: Consolas, Monaco, 'Andale Mono', monospace;
            display: block;
            margin: 20px 0;
            font-size: 14px;
        }
        .chart-container {
            margin: 30px 0;
            text-align: center;
            background-color: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 15px rgba(0, 0, 0, 0.1);
        }
        .highlight {
            background-color: #e8f5e9;
            padding: 15px;
            border-left: 5px solid #4caf50;
            margin: 20px 0;
            border-radius: 0 8px 8px 0;
        }
        .pros-cons {
            display: flex;
            gap: 20px;
            margin: 25px 0;
        }
        .pros, .cons {
            flex: 1;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0, 0, 0, 0.1);
        }
        .pros {
            background-color: #e8f5e9;
            border-left: 5px solid #4caf50;
        }
        .cons {
            background-color: #ffebee;
            border-left: 5px solid #f44336;
        }
        .metric-card {
            background-color: white;
            border-radius: 8px;
            padding: 20px;
            margin: 15px 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .metric-value {
            font-size: 28px;
            font-weight: bold;
            color: #2c3e50;
        }
        .metric-name {
            font-size: 14px;
            color: #7f8c8d;
            text-transform: uppercase;
            letter-spacing: 1px;
        }
        .model-section {
            border: 1px solid #ddd;
            border-radius: 8px;
            padding: 25px;
            margin: 30px 0;
            background-color: white;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        .timestamp {
            color: #7f8c8d;
            font-style: italic;
            margin-top: 50px;
            text-align: right;
            padding: 10px;
            border-top: 1px solid #eee;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 10px;
            box-shadow: 0 2px 20px rgba(0, 0, 0, 0.1);
        }
        .performance-table {
            width: 80%;
            margin: 30px auto;
            font-size: 16px;
        }
        .performance-table .winner {
            font-weight: bold;
            color: #2ecc71;
        }        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 2px 15px rgba(0, 0, 0, 0.1);
            display: block;
            margin: 0 auto;
        }
        /* Custom style for model comparison table */
        .model-comparison-table {
            width: 80%;
            margin: 30px auto;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
            overflow: hidden;
        }
        .model-comparison-table th {
            padding: 15px;
            background-color: #1a237e;
            color: white;
            text-align: center;
            font-weight: bold;
        }
        .model-comparison-table td {
            padding: 12px 15px;
            text-align: center;
        }
        .model-comparison-table tr:first-child th:first-child {
            background-color: #2c3e50;
        }
        .xgboost-value {
            color: #3498db;
            font-weight: bold;
        }
        .lightgbm-value {
            color: #2ecc71;
            font-weight: bold;
        }
        .winner-cell {
            background-color: rgba(46, 204, 113, 0.1);
        }
    </style>
</head>
<body>
    <div class="container">
        <header class="mb-5">
            <h1>Stock Prediction Model Training Report</h1>
            <p class="lead">Comprehensive analysis of XGBoost and LightGBM models for stock price prediction</p>        </header>
        
        <div class="chart-container">
            <img src="model_comparison_chart.png" alt="Model Performance Comparison" class="img-fluid">
        </div>
        
        <!-- Custom Performance Metrics Table -->
        <div class="model-comparison-table">
            <table class="table table-striped">
                <thead>
                    <tr>
                        <th>Metric</th>
                        <th>XGBoost</th>
                        <th>LightGBM</th>
                        <th>Winner</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>ROC AUC</strong></td>
                        <td class="xgboost-value">0.5056</td>
                        <td class="lightgbm-value winner-cell">0.5074</td>
                        <td>LightGBM</td>
                    </tr>
                    <tr>
                        <td><strong>F1 Score</strong></td>
                        <td class="xgboost-value winner-cell">0.6564</td>
                        <td class="lightgbm-value">0.6269</td>
                        <td>XGBoost</td>
                    </tr>
                    <tr>
                        <td><strong>Accuracy</strong></td>
                        <td class="xgboost-value">0.4937</td>
                        <td class="lightgbm-value winner-cell">0.4963</td>
                        <td>LightGBM</td>
                    </tr>
                    <tr>
                        <td><strong>Precision</strong></td>
                        <td class="xgboost-value">0.4917</td>
                        <td class="lightgbm-value winner-cell">0.4920</td>
                        <td>LightGBM</td>
                    </tr>
                    <tr>
                        <td><strong>Recall</strong></td>
                        <td class="xgboost-value winner-cell">0.9870</td>
                        <td class="lightgbm-value">0.8638</td>
                        <td>XGBoost</td>
                    </tr>
                    <tr>
                        <td><strong>Training Time</strong></td>
                        <td class="xgboost-value">329.25s</td>
                        <td class="lightgbm-value winner-cell">277.12s</td>
                        <td>LightGBM</td>
                    </tr>
                </tbody>
            </table>
        </div>
        
        <div class="content">
            <h1>Model Training Report</h1>
<p>Date: 2025-06-03 18:04</p>
<h2>Model Performance Summary</h2>
<pre><code class="code-block">  model  accuracy  precision    recall        f1   roc_auc
</code></pre>
<p>1  lightgbm  0.496275   0.491986  0.863816  0.626913  0.507437
0   xgboost  0.493722   0.491692  0.986950  0.656380  0.505619</p>
<h2>Best Model</h2>
<p>Model: lightgbm
Parameters: {'num_leaves': 31, 'n_estimators': 200, 'min_child_samples': 50, 'max_depth': 5, 'learning_rate': 0.1, 'feature_fraction': 0.7, 'bagging_freq': 1, 'bagging_fraction': 0.9}</p>
<h2>Feature Selection</h2>
<h1>Detailed Stock Prediction Model Training Report</h1>
<h2>Overview of Training Configuration</h2>
<p>The model training was executed with the following configuration:
- Enhanced features enabled
- Models: XGBoost and LightGBM
- Optimization strategy: Random search
- Feature importance selection enabled
- Caching enabled
- Parallel processing enabled</p>
<h2>Data Preparation and Feature Engineering</h2>
<h3>Data Source</h3>
<ul>
<li>The training utilized S&amp;P 500 stock data spanning from March 2010 to May 2025</li>
<li>494 stocks were processed, resulting in 1,833,759 stock records</li>
</ul>
<h3>Feature Engineering Process</h3>
<h4>Basic Features</h4>
<p>Initial dataset contained 106 features before any feature selection. These features included:
- Price-based features (Returns over different timeframes, Momentum indicators)
- Technical indicators (Moving Averages, RSI, Bollinger Bands)
- Volatility measures
- Price Z-scores</p>
<h4>Enhanced Features</h4>
<p>The model leveraged several feature groups:
- Technical features (19 features)
- Volume-based features (9 features)
- Volatility indicators (11 features)
- Pattern recognition (1 feature)
- Market microstructure features (5 features)
- Time-based features (4 features)
- Cross-sectional features (3 features)</p>
<h4>Feature Selection</h4>
<ul>
<li>Feature importance selection reduced the feature count from 155 to 77 features (50.3% reduction)</li>
<li>This optimization helped reduce model complexity while maintaining predictive power</li>
</ul>
<h3>Data Splitting Methodology</h3>
<p>The model used an event-aware split with the following characteristics:
- Train period: March 24, 2010 to June 24, 2022
- Test period: June 30, 2022 to May 30, 2025
- Gap between train and test: 6 days (to prevent lookahead bias)
- Train size: 1,443,499 records (80%)
- Test size: 361,608 records (20%)
- The test period included the banking crisis of 2023 (March 8-31, 2023)</p>
<h2>Model Training Methods</h2>
<h3>XGBoost Model</h3>
<h4>Training Details</h4>
<ul>
<li>Training time: 328.93 seconds</li>
<li>Best iteration: 56</li>
<li>Optimization method: Random search over 50 trials</li>
<li>Early stopping used with 10 rounds patience</li>
</ul>
<h4>Performance Metrics</h4>
<ul>
<li>ROC AUC: 0.5152 (training) / 0.5056 (test)</li>
<li>F1 Score: 0.6670 (training) / 0.6564 (test)</li>
<li>Accuracy: 0.5045 (training) / 0.4937 (test)</li>
<li>Precision: 0.5033 (training) / 0.4917 (test)</li>
<li>Recall: 0.9884 (training) / 0.9870 (test)</li>
</ul>
<div class="pros"><h4>Pros of XGBoost</h4>
<ol>
<li><strong>High Recall</strong>: Excellent at identifying positive instances (98.7% recall on test data)</li>
<li><strong>Robustness</strong>: Handles non-linear relationships well</li>
<li><strong>Feature Importance</strong>: Provides clear rankings of feature importance</li>
<li><strong>Regularization</strong>: Built-in regularization helps prevent overfitting</li>
</ol>
</div><div class="cons"><h4>Cons of XGBoost</h4>
<ol>
<li><strong>Low Precision</strong>: While recall is high, precision is low (49.17%), indicating many false positives</li>
<li><strong>Complexity</strong>: More complex to tune and optimize compared to simpler models</li>
<li><strong>Computationally Intensive</strong>: Longer training time compared to LightGBM</li>
<li><strong>Limited Accuracy</strong>: Overall accuracy slightly below 50%, indicating challenges in predicting stock movements</li>
</ol>
<h3>LightGBM Model</h3>
</div><h4>Training Details</h4>
<ul>
<li>Training time: 276.14 seconds</li>
<li>Best iteration: 19</li>
<li>Optimization method: Random search over 50 trials</li>
<li>Early stopping with 10 rounds patience</li>
</ul>
<h4>Performance Metrics</h4>
<ul>
<li>ROC AUC: 0.5164 (training) / 0.5074 (test)</li>
<li>F1 Score: 0.6366 (training) / 0.6269 (test)</li>
<li>Accuracy: 0.5081 (training) / 0.4963 (test)</li>
<li>Precision: 0.5059 (training) / 0.4920 (test)</li>
<li>Recall: 0.8583 (training) / 0.8638 (test)</li>
</ul>
<div class="pros"><h4>Pros of LightGBM</h4>
<ol>
<li><strong>Speed</strong>: Faster training time compared to XGBoost (277.12 vs 329.25 seconds)</li>
<li><strong>Memory Efficiency</strong>: Uses leaf-wise growth instead of level-wise, requiring less memory</li>
<li><strong>Better ROC AUC</strong>: Slightly higher ROC AUC score on test data (0.5074 vs 0.5056)</li>
<li><strong>Better Balanced Performance</strong>: More balanced precision-recall tradeoff than XGBoost</li>
</ol>
</div><div class="cons"><h4>Cons of LightGBM</h4>
<ol>
<li><strong>Lower Recall</strong>: Lower recall compared to XGBoost (86.38% vs 98.70%)</li>
<li><strong>Similar Accuracy Limitations</strong>: Still below 50% accuracy on test data</li>
<li><strong>Similar Precision Issues</strong>: Precision around 49%, indicating high false positive rate</li>
<li><strong>Fewer Iterations</strong>: Converged at 19 iterations vs 56 for XGBoost, possibly indicating less fine-tuning</li>
</ol>
<h2>Optimization Methods and Memory Management</h2>
<h3>Random Optimization</h3>
<ul>
<li>Performed 50 random trials for hyperparameter tuning</li>
<li>Advantages: Explores parameter space efficiently without grid search exhaustion</li>
<li>Disadvantages: May miss optimal parameter combinations</li>
</ul>
<h3>Memory Optimization</h3>
<ul>
<li>Initial data memory usage: 440.96 MB</li>
<li>After optimization: 297.34 MB (32.57% reduction)</li>
<li>Enhanced feature dataset memory: 730.76 MB reduced to 566.88 MB (22.43% reduction)</li>
</ul>
<h3>Parallel Processing</h3>
<ul>
<li>Enabled batch processing of symbols in parallel</li>
<li>Split into 10 batches of approximately 50 symbols each</li>
<li>Significantly accelerated feature generation</li>
</ul>
<h2>Detailed Comparison of Models</h2>
<h3>Performance Metrics Comparison</h3>
<table class="table table-striped model-comparison-table">
<thead>
<tr>
<th>Metric</th>
<th>XGBoost</th>
<th>LightGBM</th>
<th>Winner</th>
</tr>
</thead>
<tbody>
<tr>
<td>ROC AUC</td>
<td>0.5056</td>
<td class="lightgbm-value winner-cell">0.5074</td>
<td>LightGBM</td>
</tr>
<tr>
<td>F1 Score</td>
<td class="xgboost-value winner-cell">0.6564</td>
<td>0.6269</td>
<td>XGBoost</td>
</tr>
<tr>
<td>Accuracy</td>
<td>0.4937</td>
<td class="lightgbm-value winner-cell">0.4963</td>
<td>LightGBM</td>
</tr>
<tr>
<td>Precision</td>
<td>0.4917</td>
<td class="lightgbm-value winner-cell">0.4920</td>
<td>LightGBM</td>
</tr>
<tr>
<td>Recall</td>
<td class="xgboost-value winner-cell">0.9870</td>
<td>0.8638</td>
<td>XGBoost</td>
</tr>
<tr>
<td>Training Time</td>
<td>329.25s</td>
<td class="lightgbm-value winner-cell">277.12s</td>
<td>LightGBM</td>
</tr>
</tbody>
</table>
<h3>Prediction Tendencies</h3>
<ol>
<li><strong>XGBoost Tendency</strong>:</li>
<li>High recall but lower precision suggests XGBoost tends to predict positive stock movements frequently</li>
<li>This creates a "bullish bias" - would catch most upward movements but generates many false positives</li>
<li>
<p>Better suited for scenarios where missing positive movements is costly</p>
</li>
<li>
<p><strong>LightGBM Tendency</strong>:</p>
</li>
<li>More balanced precision-recall tradeoff</li>
<li>Slightly better at avoiding false positives</li>
<li>Potentially more useful for balanced trading strategies</li>
</ol>
<h2>Challenges and Limitations</h2>
<ol>
<li><strong>Limited Predictive Power</strong>:</li>
<li>Both models achieved AUC scores only slightly above 0.5 (random guessing)</li>
<li>
<p>Accuracy below 50% indicates persistent challenges in stock movement prediction</p>
</li>
<li>
<p><strong>Market Complexity</strong>:</p>
</li>
<li>The inclusion of the 2023 banking crisis in test data may have impacted model performance</li>
<li>
<p>Technical features alone may not capture all market dynamics</p>
</li>
<li>
<p><strong>Feature Engineering Tradeoffs</strong>:</p>
</li>
<li>Feature selection reduced features by 50.3%, which may have removed some predictive signals</li>
<li>
<p>The balance between feature count and model complexity remains challenging</p>
</li>
<li>
<p><strong>Optimization Limitations</strong>:</p>
</li>
<li>Random search may not have found optimal hyperparameters</li>
<li>Limited to 50 trials due to computational constraints</li>
</ol>
<h2>Conclusion and Recommendations</h2>
<h3>Model Selection</h3>
<ul>
<li><strong>LightGBM</strong> appears marginally better for general stock prediction with slightly higher AUC and accuracy</li>
<li><strong>XGBoost</strong> may be preferred when recall is prioritized over precision (e.g., when missing upward movements is costly)</li>
</ul>
<h3>Potential Improvements</h3>
<ol>
<li><strong>Feature Enhancement</strong>:</li>
<li>Incorporate more fundamental data and market sentiment features</li>
<li>
<p>Explore more regime-specific features for different market conditions</p>
</li>
<li>
<p><strong>Model Tuning</strong>:</p>
</li>
<li>Consider Bayesian optimization instead of random search</li>
<li>
<p>Experiment with ensemble approaches combining both models</p>
</li>
<li>
<p><strong>Data Preprocessing</strong>:</p>
</li>
<li>Further investigation into handling the banking crisis period</li>
<li>
<p>Consider more sophisticated train-test split strategies</p>
</li>
<li>
<p><strong>Prediction Strategy</strong>:</p>
</li>
<li>Develop separate models for different market regimes</li>
<li>Explore classification thresholds to better balance precision and recall</li>
</ol>
<h3>Final Assessment</h3>
<p>Both models demonstrate the inherent difficulty in predicting stock price movements. The slightly-above-random AUC scores highlight the challenge, but also suggest there is some predictive signal being captured. The choice between XGBoost and LightGBM depends on the specific trading strategy and risk tolerance, with LightGBM offering a more balanced approach and XGBoost providing higher sensitivity to upward movements at the cost of more false positives.</p></div>
        </div>
        
        <div class="timestamp">
            Report generated on 2025-06-03 18:19:55
        </div>
    </div>
    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0-alpha1/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>
